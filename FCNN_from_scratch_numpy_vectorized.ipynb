{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.genfromtxt('MNIST_CSV/mnist_train.csv', delimiter=',',dtype='int')\n",
    "test = np.genfromtxt('MNIST_CSV/mnist_test.csv', delimiter=',',dtype='int')\n",
    "\n",
    "X_train = train[:,1:].reshape(-1,1,28,28)\n",
    "y_train = train[:,0]\n",
    "\n",
    "X_test = test[:,1:].reshape(-1,1,28,28)\n",
    "y_test = test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class LayerInput(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'\n",
    "        \n",
    "class LayerDense(Layer):\n",
    "\n",
    "    def __init__(self,n_neurons,activation='linear'):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation = activation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}: activation = {self.activation}'\n",
    "    \n",
    "    def initialize_weights_and_biases(self,n_neurons_prev):\n",
    "        self.W = np.random.rand(self.n_neurons, n_neurons_prev) - 0.5\n",
    "        self.b = np.random.rand(self.n_neurons, 1) - 0.5\n",
    "\n",
    "    def update_weights_and_biases(self,dW,db,lr=0.01):\n",
    "        self.W -= lr*dW\n",
    "        self.b -= lr*db\n",
    "        \n",
    "class LayerFlatten(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = 'linear'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "\n",
    "    def __init__(self,file=None,layers=None):\n",
    "        if file:\n",
    "            self.load(file)\n",
    "        elif layers:\n",
    "            self.layers = layers\n",
    "        else:\n",
    "            self.layers = [LayerInput()]\n",
    "            self.score = 0.0\n",
    "            self.best_score = 0.0\n",
    "            self.best = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}'\n",
    "\n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def compile(self,X_shape):\n",
    "        self.activations = []\n",
    "        if len(X_shape) == 4:\n",
    "            _, channels, height, width = X_shape\n",
    "            input_flat = False\n",
    "        elif len(X_shape) == 2:\n",
    "            n_neurons, _ = X_shape\n",
    "            input_flat = True\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            self.activations.append(layer.activation)\n",
    "            if type(layer) == LayerInput:\n",
    "                layer.input_shape = (n_neurons, None) if input_flat else (None, channels, height, width)\n",
    "                layer.output_shape = (n_neurons, None) if input_flat else (None, channels, height, width)\n",
    "                layer.n_trainable_parameters = 0\n",
    "            if type(layer) == LayerFlatten:\n",
    "                n_neurons = channels*height*width\n",
    "                layer.input_shape = (None, channels, height, width)\n",
    "                layer.output_shape = (n_neurons, None)\n",
    "                layer.n_trainable_parameters = 0\n",
    "                n_neurons, _ = layer.output_shape\n",
    "            if type(layer) == LayerDense:\n",
    "                layer.initialize_weights_and_biases(n_neurons)\n",
    "                layer.input_shape = (n_neurons, None)\n",
    "                layer.output_shape = (layer.n_neurons, None)\n",
    "                layer.n_trainable_parameters = layer.n_neurons * (n_neurons + 1)\n",
    "                n_neurons, _ = layer.output_shape\n",
    "        self.describe()\n",
    "                \n",
    "    def describe(self):\n",
    "        print(f' -----------------------------------------------------------------------------------------------------------------')\n",
    "        print(f'| Layer |     Layer type      |      Input shape      |      Output shape     | Activation | Trainable Parameters |')\n",
    "        print(f' -----------------------------------------------------------------------------------------------------------------')\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            if type(layer) == LayerDense:\n",
    "                layer_type = \"{:^21}\".format('LayerDense')\n",
    "            elif type(layer) == LayerFlatten:\n",
    "                layer_type = \"{:^21}\".format('LayerFlatten')\n",
    "            elif type(layer) == LayerInput:\n",
    "                layer_type = \"{:^21}\".format('LayerInput')\n",
    "            input_shape = \"{:^23}\".format(str(layer.input_shape))\n",
    "            output_shape = \"{:^23}\".format(str(layer.output_shape))\n",
    "            n_trainable_parameters = \"{:^22}\".format(layer.n_trainable_parameters)\n",
    "            activation = \"{:^12}\".format(layer.activation)\n",
    "            print(f'|{\"{:^7}\".format(i)}|{layer_type}|{input_shape}|{output_shape}|{activation}|{n_trainable_parameters}|')\n",
    "        print(f' -----------------------------------------------------------------------------------------------------------------') \n",
    "\n",
    "    def one_hot(self,Y):\n",
    "        Y_gt = np.zeros((Y.size, self.layers[-1].n_neurons))\n",
    "        Y_gt[np.arange(Y.size), Y] = 1\n",
    "        return Y_gt.T\n",
    "\n",
    "    def ReLu(self,A):\n",
    "        return np.maximum(A,0)\n",
    "    \n",
    "    def dReLu(self,Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def SoftMax(self,A):\n",
    "        return np.exp(A) / sum(np.exp(A))\n",
    "    \n",
    "    def activate(self,A,activation):\n",
    "        if activation == 'relu':\n",
    "            return self.ReLu(A)\n",
    "        if activation == 'softmax':\n",
    "            return self.SoftMax(A)\n",
    "        if activation == 'linear':\n",
    "            return A\n",
    "        \n",
    "    def dActivation(self,Z,activation):\n",
    "        if activation == 'relu':\n",
    "            return self.dReLu(Z)\n",
    "        if activation == 'linear':\n",
    "            return 1\n",
    "\n",
    "    def forward_flatten(self,A):\n",
    "        batch_size, _, _, _ = A.shape\n",
    "        return A.reshape(batch_size,-1).T\n",
    "    \n",
    "    def forward_dense(self,A,Z,layer,is_training=False):\n",
    "        if is_training:\n",
    "            layer.cache = (A,Z)\n",
    "        Z = layer.W.dot(A) + layer.b\n",
    "        A = self.activate(Z,layer.activation)\n",
    "        return A, Z\n",
    "    \n",
    "    def backward_dense(self,dA_prev,layer,batch_size,lr,activation):\n",
    "        A_prev, Z_prev = layer.cache\n",
    "        dW = 1/batch_size * dA_prev.dot(A_prev.T)\n",
    "        db = 1/batch_size * np.sum(dA_prev)\n",
    "        dA = layer.W.T.dot(dA_prev) * self.dActivation(Z_prev,activation)\n",
    "        layer.update_weights_and_biases(dW,db,lr)\n",
    "        layer.cache = None\n",
    "        return dA\n",
    "\n",
    "    def forward(self,A,is_training=False):\n",
    "        Z = A.copy()\n",
    "        for layer in self.layers:\n",
    "            if type(layer) == LayerFlatten:\n",
    "                A = self.forward_flatten(A)\n",
    "            elif type(layer) == LayerDense:\n",
    "                A, Z = self.forward_dense(A,Z,layer,is_training)\n",
    "        return A\n",
    "    \n",
    "    def backward(self,dA,batch_size,lr):\n",
    "        for layer, activation in zip(self.layers[1:][::-1],self.activations[:-1][::-1]):\n",
    "            if type(layer) == LayerDense:\n",
    "                dA = self.backward_dense(dA,layer,batch_size,lr,activation)\n",
    "        \n",
    "    def fit(self,X_train,y_train,X_test=None,y_test=None,batch_size=100,epochs=10,lr=0.01,verbose=True):\n",
    "        self.print_progress(verbose,'start')\n",
    "        for epoch in range(epochs):\n",
    "            num_batches = X_train.shape[0] // batch_size\n",
    "            for i in range(0,X_train.shape[0],batch_size):\n",
    "                X_i = X_train[i:i+batch_size]\n",
    "                y_i = y_train[i:i+batch_size]\n",
    "                y_pred_oh = self.forward(X_i,is_training=True)\n",
    "                y_true_oh = self.one_hot(y_i)\n",
    "                dA = y_pred_oh - y_true_oh\n",
    "                self.backward(dA,batch_size,lr)\n",
    "                self.print_progress(verbose,'batch',epoch=epoch,i=i,num_batches=num_batches,batch_size=batch_size,y_i=y_i,y_pred_oh=y_pred_oh)\n",
    "            train_acc = self.val(X_train,y_train)\n",
    "            if (X_test is not None) and (y_test is not None):\n",
    "                test_acc = self.val(X_test,y_test)\n",
    "                self.print_progress(verbose,'epoch',epoch=epoch,train_acc=train_acc,test_acc=test_acc)\n",
    "                self.best_score = max(self.best_score,test_acc)\n",
    "                self.score = test_acc\n",
    "            else:\n",
    "                self.print_progress(verbose,'epoch',epoch=epoch,train_acc=train_acc,test_acc=None)\n",
    "                self.best_score = max(self.best_score,train_acc)\n",
    "                self.score = train_acc\n",
    "        self.print_progress(verbose,'end')\n",
    "\n",
    "    def print_progress(self,verbose,which,epoch=None,i=None,num_batches=None,batch_size=None,y_i=None,y_pred_oh=None,train_acc=None,test_acc=None):\n",
    "        if verbose:\n",
    "            if which == 'start':\n",
    "                print(f'{\"{:^7}\".format(\"epoch\")} | {\"{:^12}\".format(\"progress\")} | {\"{:^10}\".format(\"accuracy\")}')\n",
    "                print(f'{\"{:<53}\".format(\"-\"*53)}')\n",
    "            elif which == 'batch':\n",
    "                print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*int(i*10/num_batches/batch_size))}] | {\"{:^7}\".format(str(round(self.evaluate_accuracy(y_i,np.argmax(y_pred_oh,axis=0)),4)))}',end='\\r', flush=True)\n",
    "            elif which == 'epoch':\n",
    "                if test_acc is not None:\n",
    "                    print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*10)}] | train: {\"{:^7}\".format(str(round(train_acc,4)))} test: {\"{:^7}\".format(str(round(test_acc,4)))}')\n",
    "                else:\n",
    "                    print(f'{\"{:^7}\".format(str(epoch))} | [{\"{:<10}\".format(\"#\"*10)}] | train: {\"{:^7}\".format(str(round(train_acc,4)))}')\n",
    "            elif which == 'end':\n",
    "                print(f'{\"{:<53}\".format(\"-\"*53)}')\n",
    "\n",
    "    def predict(self,X):\n",
    "        y = self.forward(X)\n",
    "        return np.argmax(y,0)\n",
    "\n",
    "    def evaluate_accuracy(self,y_true,y_pred):\n",
    "        return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "    def val(self,X_test,y_true):\n",
    "        y_pred = self.predict(X_test)\n",
    "        acc = self.evaluate_accuracy(y_true,y_pred)\n",
    "        return acc\n",
    "                \n",
    "    def save(self,file):\n",
    "        if not file.endswith('.npz'):\n",
    "            file = file +'.npz'\n",
    "        np.savez(file=file,\n",
    "                 layers=self.layers,\n",
    "                 activations=self.activations,\n",
    "                 score = self.score,\n",
    "                 best_score=self.best_score)\n",
    "        print(f'Saved as {file}')\n",
    "        \n",
    "    def load(self,file):\n",
    "        if not file.endswith('.npz'):\n",
    "            file = file + '.npz'\n",
    "            \n",
    "        model_archieve = np.load(file,allow_pickle=True)\n",
    "        self.layers = model_archieve['layers'].tolist()\n",
    "        self.activations = model_archieve['activations'].tolist()\n",
    "        self.score = float(model_archieve['score'])\n",
    "        self.best_score = float(model_archieve['best_score'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -----------------------------------------------------------------------------------------------------------------\n",
      "| Layer |     Layer type      |      Input shape      |      Output shape     | Activation | Trainable Parameters |\n",
      " -----------------------------------------------------------------------------------------------------------------\n",
      "|   0   |     LayerInput      |   (None, 1, 28, 28)   |   (None, 1, 28, 28)   |   linear   |          0           |\n",
      "|   1   |    LayerFlatten     |   (None, 1, 28, 28)   |      (784, None)      |   linear   |          0           |\n",
      "|   2   |     LayerDense      |      (784, None)      |      (112, None)      |    relu    |        87920         |\n",
      "|   3   |     LayerDense      |      (112, None)      |      (16, None)       |    relu    |         1808         |\n",
      "|   4   |     LayerDense      |      (16, None)       |      (10, None)       |  softmax   |         170          |\n",
      " -----------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = FullyConnected()\n",
    "\n",
    "model.add_layer(LayerFlatten())\n",
    "model.add_layer(LayerDense(112,activation='relu'))\n",
    "model.add_layer(LayerDense(16,activation='relu'))\n",
    "model.add_layer(LayerDense(10,activation='softmax'))\n",
    "\n",
    "model.compile(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch  |   progress   |  accuracy \n",
      "-----------------------------------------------------\n",
      "   0    | [##########] | train: 0.1278  test: 0.1303 \n",
      "   1    | [##########] | train: 0.1812  test: 0.1853 \n",
      "   2    | [##########] | train: 0.2442  test: 0.2514 \n",
      "   3    | [##########] | train: 0.2693  test: 0.2752 \n",
      "   4    | [##########] | train: 0.3092  test: 0.3212 \n",
      "   5    | [##########] | train: 0.3653  test: 0.3839 \n",
      "   6    | [##########] | train:  0.425  test: 0.4488 \n",
      "   7    | [##########] | train: 0.4769  test: 0.5002 \n",
      "   8    | [##########] | train: 0.5232  test: 0.5459 \n",
      "   9    | [##########] | train: 0.5602  test: 0.5794 \n",
      "  10    | [##########] | train: 0.5883  test: 0.6058 \n",
      "  11    | [##########] | train: 0.6126  test: 0.6326 \n",
      "  12    | [##########] | train:  0.634  test: 0.6559 \n",
      "  13    | [##########] | train: 0.6542  test: 0.6738 \n",
      "  14    | [##########] | train: 0.6699  test: 0.6894 \n",
      "  15    | [##########] | train: 0.6763  test: 0.6928 \n",
      "  16    | [##########] | train: 0.6828  test: 0.6974 \n",
      "  17    | [##########] | train: 0.6941  test: 0.7066 \n",
      "  18    | [##########] | train: 0.7049  test: 0.7142 \n",
      "  19    | [##########] | train: 0.7143  test: 0.7206 \n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train/256,y_train,X_test/256,y_test,batch_size=10000,epochs=20,lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
